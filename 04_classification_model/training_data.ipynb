{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c02dd5ec-bd1b-4b36-b38a-3bda4e8886ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import joblib\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from Bio import Entrez\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device set to: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f7434e9-e89b-487a-a9cb-5a136cc08671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 37233341 rows from CITATION.csv\n",
      "    PMID       ISSN           DP       EDAT  PYEAR\n",
      "0      1  0006-2944     1975 Jun   1975-6-1   1975\n",
      "1     10  1873-2968  1975 Sep 01   1975-9-1   1975\n",
      "2    100  0547-6844         1975   1975-1-1   1975\n",
      "3   1000  0264-6021     1975 Sep   1975-9-1   1975\n",
      "4  10000  0006-3002  1976 Sep 28  1976-9-28   1976\n"
     ]
    }
   ],
   "source": [
    "# Load citation data\n",
    "col_names = [\"PMID\", \"ISSN\", \"DP\", \"EDAT\", \"PYEAR\"]\n",
    "\n",
    "citation_df = pd.read_csv(\n",
    "    \"/user/work/nd23942/semmeddb/raw/CITATION.csv\",\n",
    "    encoding=\"latin1\",   \n",
    "    header=None,        \n",
    "    names=col_names       \n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(citation_df)} rows from CITATION.csv\")\n",
    "print(citation_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d28516a-30ab-4257-9f2e-cd32fe85929a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 10000 PMIDs, first 5: [8057077, 27168519, 5484088, 20203436, 27353385]\n"
     ]
    }
   ],
   "source": [
    "# Sample 10000 unique PMIDs\n",
    "sample_pmids = citation_df[\"PMID\"].dropna().astype(int).drop_duplicates().sample(n=10000, random_state=42).tolist()\n",
    "print(f\"Sampled {len(sample_pmids)} PMIDs, first 5:\", sample_pmids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d82c01cd-0666-476b-98b8-b57f42bea954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching PMID 31382024: HTTP Error 400: Bad Request\n",
      "Status\n",
      "valid      6863\n",
      "invalid    3136\n",
      "error         1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8057077</td>\n",
       "      <td>Inward rectifier (IR) K+ channels of bovine pu...</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27168519</td>\n",
       "      <td>Self-harm (SH; intentional self-poisoning or s...</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PMID                                           Abstract Status\n",
       "0   8057077  Inward rectifier (IR) K+ channels of bovine pu...  valid\n",
       "1  27168519  Self-harm (SH; intentional self-poisoning or s...  valid"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to fetch abstracts\n",
    "def fetch_abstracts(pmids):\n",
    "    records = []\n",
    "    for pmid in pmids:\n",
    "        try:\n",
    "            handle = Entrez.efetch(db=\"pubmed\", id=str(pmid), rettype=\"abstract\", retmode=\"xml\")\n",
    "            xml_data = handle.read()\n",
    "            handle.close()\n",
    "            time.sleep(0.4)  # polite waiting\n",
    "\n",
    "            # Parse XML and collect every AbstractText paragraph\n",
    "            root = ET.fromstring(xml_data)\n",
    "            article = root.find(\".//PubmedArticle\")\n",
    "            abstract_texts = []\n",
    "            if article is not None:\n",
    "                abstract_elem = article.find(\".//Abstract\")\n",
    "                if abstract_elem is not None:\n",
    "                    for node in abstract_elem.findall(\"AbstractText\"):\n",
    "                        # Some nodes have .text or may be empty; strip and append\n",
    "                        if node.text and node.text.strip():\n",
    "                            abstract_texts.append(node.text.strip())\n",
    "\n",
    "            if abstract_texts:\n",
    "                status = \"valid\"\n",
    "                # join with a space to get the full multi-paragraph abstract\n",
    "                abstract = \" \".join(abstract_texts)\n",
    "            else:\n",
    "                status = \"invalid\"\n",
    "                abstract = None\n",
    "\n",
    "            records.append({\n",
    "                \"PMID\": pmid,\n",
    "                \"Abstract\": abstract,\n",
    "                \"Status\": status\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching PMID {pmid}: {e}\")\n",
    "            records.append({\n",
    "                \"PMID\": pmid,\n",
    "                \"Abstract\": None,\n",
    "                \"Status\": \"error\"\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Fetch abstracts\n",
    "Entrez.email = \"nd23942@bristol.ac.uk\"\n",
    "\n",
    "abstract_df = fetch_abstracts(sample_pmids)\n",
    "print(abstract_df[\"Status\"].value_counts())\n",
    "abstract_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c0862a2-00f2-45c4-9761-344f0ef2d9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid abstracts for classification: 6863\n",
      "       PMID                                           Abstract Status\n",
      "0   8057077  Inward rectifier (IR) K+ channels of bovine pu...  valid\n",
      "1  27168519  Self-harm (SH; intentional self-poisoning or s...  valid\n",
      "2  20203436  Fecal samples from Ruddy Shelduck, Tadorna fer...  valid\n",
      "3  27353385  Young women, especially adolescents, often lac...  valid\n",
      "4  34657444                                [Figure: see text].  valid\n"
     ]
    }
   ],
   "source": [
    "# Filter valid abstracts\n",
    "abstract_df = abstract_df[\n",
    "    (abstract_df[\"Status\"] == \"valid\") & (abstract_df[\"Abstract\"].notnull())\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(f\"Valid abstracts for classification: {len(abstract_df)}\")\n",
    "\n",
    "# Save to CSV\n",
    "abstract_df.to_csv(\"abstracts_data.csv\", index=False)\n",
    "print(abstract_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6e520e9-6d9c-4421-8c5b-e362964e57f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "# Load API key from environment\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ae9d77b-e261-4e99-96a2-d2b6f579b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a biomedical expert. Given a full abstract, identify the sentence(s)\n",
    "that clearly describe the key factual findings or results of the study.\n",
    "Do not label background, objectives, methods, or interpretative statements.\n",
    "Return each result sentence exactly as it appears, one per line.\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_prompt(text):\n",
    "    return f\"\"\"\n",
    "Below is a biomedical abstract. Identify and return only the factual result sentences.\n",
    "Return each sentence exactly as it appears, one per line.\n",
    "\n",
    "{text}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03f7a9c2-59ca-4bd8-8af7-1a357ef23aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_gpt(abstract, max_retries=3, pause=2):\n",
    "    prompt = build_prompt(abstract)\n",
    "    for i in range(max_retries):\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\",  \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\",    \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=512,\n",
    "            )\n",
    "            txt = resp.choices[0].message.content.strip()\n",
    "            # Filter out apology\n",
    "            if txt.lower().startswith((\"i'm sorry\",\"sorry\",\"unfortunately\")):\n",
    "                return \"\", 0\n",
    "            lines = [L.strip() for L in txt.split(\"\\n\") if L.strip()]\n",
    "            return \"\\n\".join(lines), len(lines)\n",
    "        except Exception as e:\n",
    "            print(f\" GPT retry {i+1}/{max_retries} failed: {e}\")\n",
    "            time.sleep(pause)\n",
    "    return \"\", 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18b84482-920d-47d0-8ca1-96dc1021d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    paras = re.split(r'\\n+', text)\n",
    "    sents = []\n",
    "    for p in paras:\n",
    "        parts = re.split(r'(?<=[\\.!?])\\s+(?=[A-Z0-9])', p)\n",
    "        sents.extend(parts)\n",
    "    return [s.strip() for s in sents if s.strip()]\n",
    "\n",
    "\n",
    "import string\n",
    "def normalize(s):\n",
    "    return s.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).strip()\n",
    "\n",
    "def close_match(s, candidates, thresh=0.6):\n",
    "    s_norm = normalize(s)\n",
    "    for c in candidates:\n",
    "        c_norm = normalize(c)\n",
    "        if s_norm in c_norm or c_norm in s_norm:\n",
    "            return True\n",
    "        if SequenceMatcher(None, s_norm, c_norm).ratio() >= thresh:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db44e4c9-40f6-461f-a03d-4fd51370293f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying GPT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6863/6863 [4:32:08<00:00,  2.38s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for _, row in tqdm(abstract_df.iterrows(), total=len(abstract_df), desc=\"Querying GPT\"):\n",
    "    pmid     = row[\"PMID\"]\n",
    "    abstract = row[\"Abstract\"]\n",
    "    gpt_out, gpt_count = query_gpt(abstract)\n",
    "    gpt_lines = [L.lstrip(\"-â€“â€” \").strip() for L in gpt_out.split(\"\\n\") if L.strip()]\n",
    "\n",
    "    tagged_lines = []\n",
    "    for sent in split_sentences(abstract):\n",
    "        if close_match(sent, gpt_lines):\n",
    "            tagged_lines.append(f\"[GPT] {sent}\")\n",
    "        else:\n",
    "            tagged_lines.append(sent)\n",
    "\n",
    "    tagged_abstract = \" \".join(tagged_lines)\n",
    "    tag_count = tagged_abstract.count(\"[GPT]\")\n",
    "\n",
    "    results.append({\n",
    "        \"PMID\": pmid,\n",
    "        \"Abstract\": abstract,\n",
    "        \"GPT_Result_Sentences\": gpt_out,\n",
    "        \"GPT_Count\": gpt_count,\n",
    "        \"Tagged_Abstract\": tagged_abstract,\n",
    "        \"Tagged_Count\": tag_count\n",
    "    })\n",
    "    time.sleep(1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06291b95-d3ce-44da-8ee3-51892eaebf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trainning_targeted_data.csv\n",
      "       PMID  GPT_Count  Tagged_Count\n",
      "0   8057077          6             6\n",
      "1  27168519          6             5\n",
      "2  20203436          3             3\n",
      "3  27353385          6             6\n",
      "4  34657444          1             1\n",
      "Approx match rate: Precision-like = 0.995\n"
     ]
    }
   ],
   "source": [
    "df_out = pd.DataFrame(results)\n",
    "df_out.to_csv(\"trainning_targeted_data.csv\", index=False)\n",
    "print(\"Saved trainning_targeted_data.csv\")\n",
    "\n",
    "print(df_out[[\"PMID\",\"GPT_Count\",\"Tagged_Count\"]].head())\n",
    "\n",
    "avg_prec = (df_out[\"Tagged_Count\"] / df_out[\"GPT_Count\"].replace(0,1)).mean()\n",
    "avg_recall = (df_out[\"Tagged_Count\"] / df_out[\"GPT_Count\"].replace(1,1)).mean()\n",
    "print(f\"Approx match rate: Precision-like = {avg_prec:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1c3dadf-1e7b-46ff-b7ff-77d442f5f382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /user/home/nd23942/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da1cf7ae-5254-4ef4-9fc8-44ce8a1bddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"training_targeted_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "270f41c2-a9f1-4847-ac47-77005ed4fc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Processing abstracts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6863/6863 [00:14<00:00, 485.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved in training_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"ðŸ”§ Processing abstracts\"):\n",
    "    pmid = str(row.get('PMID'))\n",
    "    abstract = str(row.get('Abstract', ''))\n",
    "    tagged = str(row.get('Tagged_Abstract', ''))\n",
    "\n",
    "    # Split sentence\n",
    "    abstract_sents = split_sentences(abstract)\n",
    "    tagged_sents = split_sentences(tagged)\n",
    "    \n",
    "    finding_sents = [s.replace(\"[GPT]\", \"\").strip() for s in tagged_sents if \"[GPT]\" in s]\n",
    "\n",
    "    # Labelling\n",
    "    labels = [1 if close_match(s, finding_sents) else 0 for s in abstract_sents]\n",
    "\n",
    "    records.append({\n",
    "        \"pmid\": pmid,\n",
    "        \"sentences\": abstract_sents,\n",
    "        \"labels\": labels\n",
    "    })\n",
    "\n",
    "output_path = \"training_data.jsonl\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for record in records:\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved in {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3debe2-e06c-47f8-b384-e977cda98a76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
