{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a53a61c6-32f2-4577-904a-253eba12e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d832d05c-e613-4336-a5fb-e37ceba5b8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_df = pd.read_csv(\"../02_extract_results_CITATIONS/targeted_abstracts.csv\")\n",
    "gpt_df  = pd.read_csv(\"targeted_abstracts_gpt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e10dde5-9204-4dab-b4e1-30ffa615e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_df = bert_df.rename(columns={\"Targeted_Abstract\": \"Abstract_bert\"})\n",
    "gpt_df  = gpt_df.rename(columns={\"Targeted_Abstract\": \"Abstract_gpt\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "400be901-415a-466f-92f4-d1486457f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(\n",
    "    bert_df[[\"PMID\", \"Abstract_bert\"]],\n",
    "    gpt_df[[\"PMID\",  \"Abstract_gpt\"]],\n",
    "    on=\"PMID\",\n",
    "    how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b7facd9-8452-4313-bebe-c705a76cd09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    return [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n",
    "\n",
    "def extract_bert_sentences(text):\n",
    "    return [s.replace(\"[TAR]\", \"\").strip()\n",
    "            for s in split_sentences(text)\n",
    "            if \"[TAR]\" in s]\n",
    "\n",
    "def extract_gpt_sentences(text):\n",
    "    return [s.replace(\"[GPT]\", \"\").strip()\n",
    "            for s in split_sentences(text)\n",
    "            if \"[GPT]\" in s]\n",
    "\n",
    "def sentence_overlap(a, b, thresh=0.7):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio() >= thresh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8918320-b7bf-4972-8b4e-823b9a8b2f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = []\n",
    "total_matched = total_bert = total_gpt = 0\n",
    "zero_bert = zero_gpt = 0\n",
    "\n",
    "for _, row in merged.iterrows():\n",
    "    bert_sents = extract_bert_sentences(row[\"Abstract_bert\"])\n",
    "    gpt_sents  = extract_gpt_sentences(row[\"Abstract_gpt\"])\n",
    "    matched    = sum(\n",
    "        any(sentence_overlap(g, b) for b in bert_sents)\n",
    "        for g in gpt_sents\n",
    "    )\n",
    "\n",
    "    # counts\n",
    "    b_cnt = len(bert_sents)\n",
    "    g_cnt = len(gpt_sents)\n",
    "\n",
    "    # track zeros\n",
    "    if b_cnt == 0: zero_bert += 1\n",
    "    if g_cnt == 0: zero_gpt  += 1\n",
    "\n",
    "    # per‚Äêpaper precision/recall\n",
    "    prec = matched / b_cnt if b_cnt else 0.0\n",
    "    rec  = matched / g_cnt if g_cnt else 0.0\n",
    "    f1   = (2 * prec * rec) / (prec + rec) if (prec + rec) else 0.0\n",
    "\n",
    "    stats.append({\n",
    "        \"PMID\":       row[\"PMID\"],\n",
    "        \"BERT_count\": b_cnt,\n",
    "        \"GPT_count\":  g_cnt,\n",
    "        \"Matched\":    matched,\n",
    "        \"Precision\":  round(prec, 4),\n",
    "        \"Recall\":     round(rec, 4),\n",
    "        \"F1\":         round(f1, 4)\n",
    "    })\n",
    "\n",
    "    # accumulate for micro\n",
    "    total_matched += matched\n",
    "    total_bert   += b_cnt\n",
    "    total_gpt    += g_cnt\n",
    "\n",
    "df_stats = pd.DataFrame(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e02dbf97-7423-46b1-8737-fe8bbf8ed22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_p = df_stats[\"Precision\"].mean()\n",
    "macro_r = df_stats[\"Recall\"].mean()\n",
    "macro_f1= df_stats[\"F1\"].mean()\n",
    "\n",
    "micro_p = total_matched / total_bert if total_bert else 0.0\n",
    "micro_r = total_matched / total_gpt  if total_gpt  else 0.0\n",
    "micro_f1= (2 * micro_p * micro_r) / (micro_p + micro_r) if (micro_p + micro_r) else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebd0772c-fb97-464d-9d94-cf06b90afab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers processed: 693\n",
      "Papers w/ zero BERT tags: 72\n",
      "Papers w/ zero GPT tags : 11\n",
      "\n",
      "=== Macro-averaged scores ===\n",
      "Precision: 0.5778\n",
      "Recall   : 0.7002\n",
      "F1 Score : 0.5981\n",
      "\n",
      "=== Micro-averaged scores ===\n",
      "Precision: 0.6373\n",
      "Recall   : 0.7425\n",
      "F1 Score : 0.6859\n",
      "\n",
      "Saved bert_vs_gpt_stats.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"Papers processed: {len(df_stats)}\")\n",
    "print(f\"Papers w/ zero BERT tags: {zero_bert}\")\n",
    "print(f\"Papers w/ zero GPT tags : {zero_gpt}\\n\")\n",
    "\n",
    "print(\"=== Macro-averaged scores ===\")\n",
    "print(f\"Precision: {macro_p:.4f}\")\n",
    "print(f\"Recall   : {macro_r:.4f}\")\n",
    "print(f\"F1 Score : {macro_f1:.4f}\\n\")\n",
    "\n",
    "print(\"=== Micro-averaged scores ===\")\n",
    "print(f\"Precision: {micro_p:.4f}\")\n",
    "print(f\"Recall   : {micro_r:.4f}\")\n",
    "print(f\"F1 Score : {micro_f1:.4f}\")\n",
    "\n",
    "df_stats.to_csv(\"bert_vs_gpt_stats.csv\", index=False)\n",
    "print(\"\\nSaved bert_vs_gpt_stats.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
